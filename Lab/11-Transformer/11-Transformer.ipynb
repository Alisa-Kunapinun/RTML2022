{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Transformer\n",
    "\n",
    "Reference: \n",
    "- https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "- https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
    "- https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632\n",
    "- https://github.com/lucidrains/vit-pytorch#vision-transformer---pytorch\n",
    "- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "\n",
    "Today's lab will describe how to create transformer from scratch and how to implement it. :-)\n",
    "\n",
    "Moreover, we will continue to ViT (Vision transformer).\n",
    "\n",
    "<img src=\"img/optimus_prime.jpg\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "Note that this photo does not have relevance in the topic. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer trend\n",
    "\n",
    "In recent years, the deep learning network structures are in the form of CNNs or RNNs. CNNs are mostly used in visions, and RNNs are mostly used in NLP (Natural language processing). Their models can be worked together. CNNs can be used in language and RNNs also can be used in vision. However, both models have some disadvantage such as coreference resolution, and huge or complex dataset in languages.\n",
    "\n",
    "In 2017, The google research lab has been introduced Transformer in the paper name: \"Attention is all you need\". This paper is very famous and has been cited more than 8 thousands times.\n",
    "\n",
    "The main concept of Transformer is self-Attention process. The procees is not only replace the RNNs and CNNs, it also shows the relation between text and data.\n",
    "\n",
    "## Transformer architecture\n",
    "\n",
    "We use the code from [github](https://github.com/fkodom/transformer-from-scratch/tree/main/src)\n",
    "\n",
    "The transformer diagram is show as below:\n",
    "\n",
    "<img src=\"img/Transformer.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "The summary details and mathematics of the architecture is:\n",
    "\n",
    "<img src=\"img/SummaryTransformer.PNG\" title=\"Transformer Details\" style=\"width: 1000px;\" />\n",
    "\n",
    "There are processes that we need to implement in the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention!\n",
    "\n",
    "In present, the standard model which is used in *sequence-to-sequence learning* is **Sequence-to-Sequence Model** (seq2seq) or **RNN Encoder-Decoder**. The model combines with 2 pieces called encoder and decoder.\n",
    "The encoder receives the input and keeps some important information, and the decoder releases the output from the related information.\n",
    "\n",
    "However, the seq2seq has a problem that some information may be lost from the long sequence running. Thus, the concept of attention is **to focus at the specific input directly**.\n",
    "\n",
    "In attention, when we want to get the output at a target position, the decoder vector at the position willcaculate between the attention score and encoder vector in every position. The high score in some encoder position can tell that it is important than the other position. To get the probability weight for each encoder vector can be explained in softmax as:\n",
    "\n",
    "$$r = \\sum_i \\frac{e^{p_i\\cdot q}}{\\sum_j e^{p_j\\cdot q}}p_i$$\n",
    "\n",
    "### Multi-head attention\n",
    "\n",
    "Transformers use a specific type of attention mechanism, referred to as multi-head attention. This is the most important part of the model. An illustration from the paper is shown below.\n",
    "\n",
    "<img src=\"img/MultiHeadAttention.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "The multi-head attention layer is described as:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Q, K, and V are batches of matrices, each with shape <code>(batch_size, seq_length, num_features)</code>. Multiplying the query $Q$ and key $K$ arrays results in a <code>(batch_size, seq_length, seq_length)</code> array, which tells us roughly how important each element in the sequence is. This is the attention of this layer — it determines which elements we “pay attention” to. The attention array is normalized using softmax, so that all of the weights sum to one. Finally, the attention is applied to the value (V) array using matrix multiplication.\n",
    "\n",
    "The scaled dot-product attention code is below: (multi_head_attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    # MatMul operations are translated to torch.bmm in PyTorch\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head is composed of several identical *attention heads*. Each attention head contains 3 lineary layers and combine them using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can combine them to be multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each attention head computes its own query, key, and value arrays, and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows us to “pay attention” to more parts of the sequence at once, which makes the model more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "To completed the encoding zone of transformer (input zone), we need to build another component: **position encoder**.\n",
    "The <code>MultiHeadAttention</code> has no trainable components that operate over the *sequence dimension* (axis 1). Everything operates over the *feature dimension* (axis 2), and so it is independent of sequence length. We have to provide positional information to the model, so that it knows about the relative position of data points in the input sequences.\n",
    "\n",
    "The positional information is encoded using **trigonometric functions** as:\n",
    "\n",
    "$$PE_{(pos,2i)}=\\sin (\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "$$PE_{(pos,2i+1)}=\\cos (\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "\n",
    "This constant is a 2d matrix. Pos refers to the order in the sentence, and i refers to the position along the embedding vector dimension. Each value in the pos/i matrix is then worked out using the equations above.\n",
    "\n",
    "<img src=\"img/positionalencoder.png\" title=\"Positional Encoder\" style=\"width: 400px;\" />\n",
    "\n",
    "The positional encoding is implemented in code as: (utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),) -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim // dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should sinusoidal encodings extrapolate to longer sequence lengths? Because sine/cosine functions are periodic, and they cover a range of $[0, 1]$. Most other choices of encoding would not be periodic or restricted to the range $[0, 1]$. Suppose that, during inference, you provide an input sequence longer than any used during training. Positional encoding for the last elements in the sequence could be different than anything the model has seen before. For those reasons, and despite the fact that learned embeddings appeared to perform equally as well, the authors still chose to use sinusoidal encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The completed transformer\n",
    "\n",
    "Transformer uses an encoder-decoder architecture. The encoder processes the input sequence and returns a **feature vector (or memory vector)**. The decoder processes the **target sequence, and incorporates information from the encoder memory**. The output from the decoder is the model prediction.\n",
    "\n",
    "We need to build another piece for the transformer. That is the feed forward network. (utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine dropout and normalization, we can build residual module (utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woooo, we can create encoder now! (encoder.py)\n",
    "\n",
    "First, let's create Transformer Encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Transformer encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder module is extremely similar. Just a few small differences:\n",
    "- The decoder accepts two arguments (target and memory), rather than one.\n",
    "- There are two multi-head attention modules per layer, instead of one.\n",
    "- The second multi-head attention accepts memory for two of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine everthing, Finish!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a simple test, as a sanity check for our implementation. We can construct random tensors for src and tgt, check that our model executes without errors, and confirm that the output tensor has the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "# input (64-batch_size, 32-words, 512-embedding)\n",
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)\n",
    "# torch.Size([64, 16, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to train transformer using the PyTorch model from [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "A Vision Transformer (ViT) is a transformer that is targeted at vision processing tasks. It have dominated the field of Computer Vision, obtaining state-of-the-art performance in image classification and others.\n",
    "\n",
    "The ViT concept for image classification is shown at below:\n",
    "\n",
    "<img src=\"img/vit.gif\" title=\"ViT\" />\n",
    "\n",
    "### How the ViT works?\n",
    "\n",
    "There are steps of ViT as:\n",
    "\n",
    "1. Split an image into patches\n",
    "2. Flatten the patches\n",
    "3. Produce lower-dimensional linear embeddings from the flattened patches\n",
    "4. Add positional embeddings\n",
    "5. Feed the sequence as an input to a standard transformer encoder\n",
    "6. Pretrain the model with image labels (fully supervised on a huge dataset)\n",
    "7. Finetune on the downstream dataset for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT architecture\n",
    "\n",
    "As you know that the ViT has implemented from Transforemer, so the architecture will be the same as transformer. However, using ViT for image classification is used only the Transformer encoder. Thus, the decoder has been removed from the architecture. The ViT architecture is shown at below:\n",
    "\n",
    "<img src=\"img/ViTArchitecture.png\" title=\"ViT architecture\" />\n",
    "\n",
    "From the figure, there are 4 parts:\n",
    "<ol style=\"list-style-type:lower-alpha\">\n",
    "    <li> the main architecture of the model </li>\n",
    "    <li> the Transformer module </li>\n",
    "    <li> the Multiscale self-attention (MSA) head </li>\n",
    "    <li> The self-attention (SA) head </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start\n",
    "\n",
    "Since the goal is how to create ViT model, so we use the MNIST dataset for train and test it!\n",
    "\n",
    "We get the code example from: https://github.com/BrianPulfer/PapersReimplementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test function\n",
    "\n",
    "Create the train and test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ViT_classify(model, optimizer, N_EPOCHS, train_loader, device=\"cpu\"):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y) / len(x)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "        \n",
    "def test_ViT_classify(model, optimizer, test_loader):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        test_loss += loss\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Self Attention (MSA) Model\n",
    "\n",
    "Before create ViT model, as same as normal transformer, we need to create MSA model to complete all model.\n",
    "\n",
    "For a single image, that each patch gets updated based on some similarity measure with the other patches.\n",
    "Thus, do linearly mapping each patch (that is now an 8-dimensional vector in our example) to 3 distinct vectors: $q$, $k$, and $v$ (query, key, value).\n",
    "\n",
    "For each single patch, compute the dot product between its $q$ vector with all of the $k$ vectors, and divide by the square root of the dimension of these vectors.\n",
    "At the end of the patch, do softmax the patch ouput. This is called attention cues, and multiply each attention cue with the $v$ vectors associated with the different $k$ vectors and sum all up.\n",
    "\n",
    "Each patch is assumed to be a new value that is based on its similarity (after the linear mapping to $q$, $k$, and $v$) with other patches.\n",
    "\n",
    "However, the whole procedure is carried out $H$ times on $H$ sub-vectors of our current 8-dimensional patches, where $H$ is the number of Heads.\n",
    "\n",
    "Once all results are obtained, they are concatenated together. Finally, the result is passed through a linear layer.\n",
    "\n",
    "The MSA model is shown at below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.k_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.v_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: for each head, we create distinct Q, K, and V mapping functions (square matrices of size 4x4 in our example).\n",
    "\n",
    "Since our inputs will be sequences of size (N, 50, 8), and we only use 2 heads, we will at some point have an (N, 50, 2, 4) tensor, use a nn.Linear(4, 4) module on it, and then come back, after concatenation, to an (N, 50, 8) tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "As anticipated, positional encoding allows the model to understand where each patch would be placed in the original image. While it is theoretically possible to learn such positional embeddings, previous work by Vaswani et. al.\n",
    "\n",
    "In particular, positional encoding adds low-frequency values to the first dimensions and higher-frequency values to the latter dimensions.\n",
    "\n",
    "In each sequence, for token i we add to its j-th coordinate the following value:\n",
    "\n",
    "$$ p_{i,j} =\n",
    "\\left\\{\\begin{matrix}\n",
    "\\sin (\\frac{i}{10000^{j/d_{embdim}}})\\\\ \n",
    "\\cos (\\frac{i}{10000^{j/d_{embdim}}})\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "<img src=\"img/peimages.png\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d, device=\"cpu\"):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Model\n",
    "\n",
    "Create the ViT model as below. The explaination is later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patches sizes\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.n_heads = n_heads\n",
    "        assert input_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert input_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        # (In forward method)\n",
    "\n",
    "        # 4a) Layer normalization 1\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "        # 4b) Multi-head Self Attention (MSA) and classification token\n",
    "        self.msa = MSA(self.hidden_d, n_heads)\n",
    "\n",
    "        # 5a) Layer normalization 2\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "        # 5b) Encoder MLP\n",
    "        self.enc_mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, self.hidden_d),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 6) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, w, h = images.shape\n",
    "        patches = images.reshape(n, self.n_patches ** 2, self.input_d)\n",
    "\n",
    "        # Running linear layer for tokenization\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d, device).repeat(n, 1, 1)\n",
    "\n",
    "        # TRANSFORMER ENCODER BEGINS ###################################\n",
    "        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n",
    "        # Running Layer Normalization, MSA and residual connection\n",
    "        self.msa(self.ln1(tokens.to(\"cpu\")).to(device))\n",
    "        out = tokens + self.msa(self.ln1(tokens))\n",
    "\n",
    "        # Running Layer Normalization, MLP and residual connection\n",
    "        out = out + self.enc_mlp(self.ln2(out))\n",
    "        # TRANSFORMER ENCODER ENDS   ###################################\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        return self.mlp(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Patchifying and the linear mapping\n",
    "\n",
    "The transformer encoder was developed with sequence data in mind, such as English sentences. However, an image is not a sequence. Thus, we break it into multiple sub-images and map each sub-image to a vector.\n",
    "\n",
    "We do so by simply reshaping our input, which has size $(N, C, H, W)$ (in our example $(N, 1, 28, 28)$), to size (N, #Patches, Patch dimensionality), where the dimensionality of a patch is adjusted accordingly.\n",
    "\n",
    "In MNIST, we break each $(1, 28, 28)$ into 7x7 patches (hence, each of size 4x4). That is, we are going to obtain 7x7=49 sub-images out of a single image.\n",
    "\n",
    "$$(N,1,28,28) \\rightarrow (N,P\\times P, H \\times C/P  \\times W \\times C/P) \\rightarrow (N, 7\\times 7, 4\\times 4) \\rightarrow (N, 49, 16)$$\n",
    "\n",
    "<img src=\"img/patch.png\" title=\"an image is split into patches\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Adding the classification token\n",
    "\n",
    "When information about all other tokens will be present here, we will be able to classify the image using only this special token. The initial value of the special token (the one fed to the transformer encoder) is a parameter of the model that needs to be learned.\n",
    "\n",
    "We can now add a parameter to our model and convert our (N, 49, 8) tokens tensor to an (N, 50, 8) tensor (we add the special token to each sequence).\n",
    "\n",
    "Passing from (N,49,8) → (N,50,8) is probably sub-optimal. Also, notice that the classification token is put as the first token of each sequence. This will be important to keep in mind when we will then retrieve the classification token to feed to the final MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Positional encoding\n",
    "\n",
    "See above, as we mentioned.\n",
    "\n",
    "#### Step 4: LN, MSA, and Residual Connection\n",
    "\n",
    "The step is to apply layer normalization to the tokens, then apply MSA, and add a residual connection (add the input we had before applying LN).\n",
    "- **Layer normalization** is a popular block that, given an input, subtracts its mean and divides by the standard deviation.\n",
    "- **MSA**: same as the vanilla transformer.\n",
    "- **A residual connection** consists in just adding the original input to the result of some computation. This, intuitively, allows a network to become more powerful while also preserving the set of possible functions that the model can approximate.\n",
    "\n",
    "The residual connection is added at the original (N, 50, 8) tensor to the (N, 50, 8) obtained after LN and MSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: LN, MLP, and Residual Connection\n",
    "All that is left to the transformer encoder is just a simple residual connection between what we already have and what we get after passing the current tensor through another LN and an MLP.\n",
    "\n",
    "#### Step 6: Classification MLP\n",
    "Finally, we can extract just the classification token (first token) out of our N sequences, and use each token to get N classifications.\n",
    "Since we decided that each token is an 8-dimensional vector, and since we have 10 possible digits, we can implement the classification MLP as a simple 8x10 matrix, activated with the SoftMax function.\n",
    "\n",
    "The output of our model is now an (N, 10) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Using device', device)\n",
    "# cuda is does not work now -> kernels always died\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT((1, 28, 28), n_patches=7, hidden_d=20, n_heads=2, out_d=10)\n",
    "model = model.to(device)\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LR = 0.01\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 406.94\n",
      "Epoch 2/5 loss: 373.10\n",
      "Epoch 3/5 loss: 367.05\n",
      "Epoch 4/5 loss: 365.66\n",
      "Epoch 5/5 loss: 364.58\n"
     ]
    }
   ],
   "source": [
    "train_ViT_classify(model, optimizer, N_EPOCHS, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 60.45\n",
      "Test accuracy: 91.45%\n"
     ]
    }
   ],
   "source": [
    "test_ViT_classify(model, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy is around 90% and our implement is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch ViT\n",
    "\n",
    "[Here](https://github.com/lucidrains/vit-pytorch#vision-transformer---pytorch) is the link of the full version of ViT using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit-pytorch\n",
      "  Downloading vit_pytorch-0.29.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6 in /home/alisa/anaconda3/lib/python3.8/site-packages (from vit-pytorch) (1.8.0+cu111)\n",
      "Collecting einops>=0.4.1\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torchvision in /home/alisa/anaconda3/lib/python3.8/site-packages (from vit-pytorch) (0.9.0+cu111)\n",
      "Requirement already satisfied: numpy in /home/alisa/.local/lib/python3.8/site-packages (from torch>=1.6->vit-pytorch) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /home/alisa/.local/lib/python3.8/site-packages (from torch>=1.6->vit-pytorch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/alisa/anaconda3/lib/python3.8/site-packages (from torchvision->vit-pytorch) (7.2.0)\n",
      "Installing collected packages: einops, vit-pytorch\n",
      "Successfully installed einops-0.4.1 vit-pytorch-0.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ViT Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistillableViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
    "\n",
    "teacher = resnet50(pretrained = True)\n",
    "\n",
    "v = DistillableViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "distiller = DistillWrapper(\n",
    "    student = v,\n",
    "    teacher = teacher,\n",
    "    temperature = 3,           # temperature of distillation\n",
    "    alpha = 0.5,               # trade between main loss and distillation loss\n",
    "    hard = False               # whether to use soft or hard distillation\n",
    ")\n",
    "\n",
    "img = torch.randn(2, 3, 256, 256)\n",
    "labels = torch.randint(0, 1000, (2,))\n",
    "\n",
    "loss = distiller(img, labels)\n",
    "loss.backward()\n",
    "\n",
    "# after lots of training above ...\n",
    "\n",
    "pred = v(img) # (2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
